import os
import google.generativeai as genai
import anthropic
import json

def call_gemini_flash(llm_input: str, system_prompt: str) -> str:
    """
    Calls the Gemini Flash API to generate a response to the given input.

    Args:
        llm_input (str): A JSON string containing the input to the language model.
        system_prompt (str): The system prompt for the language model.

    Returns:
        str: The response generated by the language model.
    """

    # Configure the API key for the Gemini Flash API.
    genai.configure(api_key=os.environ["GEMINI_API_KEY"])

    # Generation configuration for the language model.
    generation_config = {
        "temperature": 0,
        "top_p": 0.95,
        "top_k": 64,
        "max_output_tokens": 8192,
        "response_mime_type": "application/json",
    }

    # Initialize the GenerativeModel with the specified model and generation configuration.
    model = genai.GenerativeModel(
        model_name="gemini-1.5-flash",
        generation_config=generation_config,
        system_instruction=system_prompt,
    )

    # Start a new chat session and send the input to the language model.
    chat_session = model.start_chat(history=[])

    # Send the input to the language model and receive the response.
    response = chat_session.send_message(llm_input)

    # Convert the JSON response to a list of dictionaries.
    try:
        response_list = json.loads(response)
    except json.JSONDecodeError as e:
        print(f"JSON decoding error: {e}")
        print(f"Response: {response}")
        return f"Error decoding JSON response: {e}"
    except Exception as e:
        print(f"An error occurred: {e}")
        print(f"Response: {response}")
        return f"An error occurred: {e}"

    # Return the response generated by the language model.
    return response_list


# def call_claude_haiku(llm_input: str, system_prompt: str) -> str:
#     client = anthropic.Anthropic(
#         # defaults to os.environ.get("ANTHROPIC_API_KEY")
#         api_key="my_api_key",
#     )

#     message = client.messages.create(
#         model="claude-3-haiku-20240307",
#         max_tokens=4096,
#         temperature=0,
#         messages=[]
#     )
#     print(message.content)
